{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premier essai avec modèle linéaire :\n",
    "* optimizer = Adam\n",
    "* callbakcs = EarlyStopping avec patience de 20\n",
    "* batch_size = 1024\n",
    "\n",
    "<img src=\"img/linear_model_1.png\">\n",
    "\n",
    "<img src=\"img/linear_model_2.png\">\n",
    "\n",
    "<img src=\"img/linear_model_3.png\">\n",
    "\n",
    "<img src=\"img/linear_model_4.png\">\n",
    "\n",
    "Arrèt au 147ème epoch -> underfitting\n",
    "\n",
    "On passe à un modèle plus complexe -> Multi-layer-perceptron\n",
    "\n",
    "Test avec :\n",
    "* 2 couches cachées de 180 neurones\n",
    "* dropout = 20 %\n",
    "* optimizer = Adam\n",
    "* callbakcs = EarlyStopping avec patience de 15\n",
    "* batch_size = 1024\n",
    "\n",
    "<img src=\"img/MLP_1.png\">\n",
    "\n",
    "<img src=\"img/MLP_2.png\">\n",
    "\n",
    "<img src=\"img/MLP_3.png\">\n",
    "\n",
    "<img src=\"img/MLP_4.png\">\n",
    "\n",
    "-> underfitting\n",
    "\n",
    "Nouveau test avec : \n",
    "* 3 couches cachées de 180 neurones\n",
    "* dropout = 10 %\n",
    "\n",
    "<img src=\"img/MLP_5.png\">\n",
    "\n",
    "<img src=\"img/MLP_6.png\">\n",
    "\n",
    "<img src=\"img/MLP_7.png\">\n",
    "\n",
    "<img src=\"img/MLP_8.png\">\n",
    "\n",
    "-> On garde la fonction d'activation elu\n",
    "\n",
    "Nouveau test avec : \n",
    "* 5 couches cachées de 180 neurones\n",
    "\n",
    "<img src=\"img/MLP_9.png\">\n",
    "\n",
    "<img src=\"img/MLP_10.png\">\n",
    "\n",
    "<img src=\"img/MLP_11.png\">\n",
    "\n",
    "<img src=\"img/MLP_12.png\">\n",
    "\n",
    "-> peu de changements\n",
    "\n",
    "Nouvel essai :\n",
    "* couche caché 580, 360, 240, 160, 80 neurones\n",
    "\n",
    "<img src=\"img/MLP_13.png\">\n",
    "\n",
    "<img src=\"img/MLP_14.png\">\n",
    "\n",
    "<img src=\"img/MLP_15.png\">\n",
    "\n",
    "<img src=\"img/MLP_16.png\">\n",
    "\n",
    "-> overfitting\n",
    "\n",
    "-> nouveau test avec 20 % de dropout\n",
    "\n",
    "<img src=\"img/MLP_17.png\">\n",
    "\n",
    "<img src=\"img/MLP_18.png\">\n",
    "\n",
    "<img src=\"img/MLP_19.png\">\n",
    "\n",
    "<img src=\"img/MLP_20.png\">\n",
    "\n",
    "-> on n'overfit plus\n",
    "-> ajout d'une couche caché de 820 neurones en première place\n",
    "\n",
    "<img src=\"img/MLP_21.png\">\n",
    "\n",
    "<img src=\"img/MLP_22.png\">\n",
    "\n",
    "<img src=\"img/MLP_23.png\">\n",
    "\n",
    "<img src=\"img/MLP_24.png\">\n",
    "\n",
    "-> overfit à nouveau\n",
    "-> dropout à 25 %\n",
    "\n",
    "<img src=\"img/MLP_25.png\">\n",
    "\n",
    "<img src=\"img/MLP_26.png\">\n",
    "\n",
    "<img src=\"img/MLP_27.png\">\n",
    "\n",
    "<img src=\"img/MLP_28.png\">\n",
    "\n",
    "-> on essaye d'avoir 60 % au testing\n",
    "-> nouvelle couche caché de 1000 neurones en première place\n",
    "-> on preshot l'overfitting avec un dropout à 28 %\n",
    "\n",
    "<img src=\"img/MLP_29.png\">\n",
    "\n",
    "<img src=\"img/MLP_30.png\">\n",
    "\n",
    "<img src=\"img/MLP_31.png\">\n",
    "\n",
    "<img src=\"img/MLP_32.png\">\n",
    "\n",
    "-> nouveau test avec le learning rate de Adam changé de 0.001 -> 0.002\n",
    "\n",
    "<img src=\"img/MLP_33.png\">\n",
    "\n",
    "<img src=\"img/MLP_34.png\">\n",
    "\n",
    "<img src=\"img/MLP_35.png\">\n",
    "\n",
    "<img src=\"img/MLP_36.png\">\n",
    "\n",
    "-> Pas vraiment de changement\n",
    "-> Passage au CNN\n",
    "\n",
    "-> Test avec 3 couches de convolutions (3x3) avec 32 filtres\n",
    "-> Max pooling\n",
    "-> 10 % de dropout pour anticiper un overfitting\n",
    "-> full connected layer : couche cachée de 32 neurones\n",
    "\n",
    "<img src=\"img/CNN_1.png\">\n",
    "\n",
    "<img src=\"img/CNN_2.png\">\n",
    "\n",
    "<img src=\"img/CNN_3.png\">\n",
    "\n",
    "<img src=\"img/CNN_4.png\">\n",
    "\n",
    "-> On choisit relu et elu\n",
    "-> nouveau test avec 3 couches de convolutions avec 64, 32 puis 16 filtres.\n",
    "\n",
    "<img src=\"img/CNN_5.png\">\n",
    "\n",
    "<img src=\"img/CNN_6.png\">\n",
    "\n",
    "<img src=\"img/CNN_7.png\">\n",
    "\n",
    "<img src=\"img/CNN_8.png\">\n",
    "\n",
    "L'algo apprenait encore, faire passer la patience de 10 à 20\n",
    "\n",
    "Après de nombreux test de mon côté je décide de me focaliser sur la fonction relu qui certes converge moins vite mais à terme donne de meilleurs résultats.\n",
    "\n",
    "Nouveau test :\n",
    "* patiente = 20\n",
    "* 2 couches cachées de 30 neurones\n",
    "\n",
    "<img src=\"img/CNN_9.png\">\n",
    "\n",
    "<img src=\"img/CNN_10.png\">\n",
    "\n",
    "<img src=\"img/CNN_11.png\">\n",
    "\n",
    "<img src=\"img/CNN_12.png\">\n",
    "\n",
    "Nouveau test avec :\n",
    "* 3 couches cachées de 64, 32, 16 neurones pour le full-connected layer\n",
    "* batch size = 512\n",
    "* patience = 30\n",
    "\n",
    "\n",
    "<img src=\"img/CNN_21.png\">\n",
    "\n",
    "<img src=\"img/CNN_22.png\">\n",
    "\n",
    "<img src=\"img/CNN_23.png\">\n",
    "\n",
    "<img src=\"img/CNN_24.png\">\n",
    "\n",
    "Les résultats sont meilleurs que ceux précédents mais est-ce du au changement du batch size ou à l'ajout de couches cachées pour le fully connected layer ?\n",
    "\n",
    "Nouveau test en supprimant les couches cachées.\n",
    "\n",
    "<img src=\"img/CNN_25.png\">\n",
    "\n",
    "<img src=\"img/CNN_26.png\">\n",
    "\n",
    "<img src=\"img/CNN_27.png\">\n",
    "\n",
    "<img src=\"img/CNN_28.png\">\n",
    "\n",
    "Comme on peut le voir, le gain n'est pas du à l'ajout des couches cachées mais à la baisse du batch size.\n",
    "\n",
    "L'avantage des grands lot de données(batch size) est que ceux ci permettent d'accélerer l'entrainement grâce au parallélisme des GPUs, la contrepartie étant que pour une raison assez méconnue cela mène à une pauvre généralisation.\n",
    "D'un autre côté des batch size de 1 ne fonctionne pas très bien\n",
    "\n",
    "Voyons l'effet du batch size sur l'entrainement pour d'autres valeurs comme 64, 128 et 256\n",
    "\n",
    "<img src=\"img/CNN_29.png\">\n",
    "\n",
    "<img src=\"img/CNN_30.png\">\n",
    "\n",
    "<img src=\"img/CNN_31.png\">\n",
    "\n",
    "<img src=\"img/CNN_32.png\">\n",
    "\n",
    "Comme on peut le voir, on obtient des résultats assez similaires, on gardera donc 256 étant donné qu'il permet un entrainement plus rapide.\n",
    "\n",
    "\n",
    "Nouveau test, cette fois ci avec :\n",
    "- 2 couches de convolution de 196 filtres puis max pooling puis dropout\n",
    "- 2 couches de convolution de 128 filtres puis max pooling puis dropout\n",
    "- 2 couches de convolution de 64 filtres puis max pooling puis dropout\n",
    "- 2 couches de convolution de 32 filtres puis max pooling puis dropout\n",
    "- 2 couches de convolution de 16 filtres puis max pooling puis dropout\n",
    "- Une couche Dense avec 10 neurones de sortie\n",
    "\n",
    "\n",
    "<img src=\"img/CNN_33.png\">\n",
    "\n",
    "<img src=\"img/CNN_34.png\">\n",
    "\n",
    "<img src=\"img/CNN_35.png\">\n",
    "\n",
    "<img src=\"img/CNN_36.png\">\n",
    "\n",
    "Comme on peut le voir le fait de dupliquer une couche en 2 plus petite semble bien fonctionner.\n",
    "\n",
    "On peut également voir un énorme overfitting.\n",
    "\n",
    "Pour combattre l'overfitting, plusieurs solutions :\n",
    "* augmenter le nombre de données (data augmentation)\n",
    "* complexifier le modèle (limité par la mémoire RAM de mon PC)\n",
    "* techniques de régalurisation (augmenter le droput de 10 % à ???)\n",
    "\n",
    "Si on raisonne sur l'architecture plus haut, utilisant les couches de convolution avec ajout de padding, les couches de convolution ne réduisent pas la taille de l'image.\n",
    "Le max pooling lui réduit par 2 la taille de l'image.\n",
    "On a donc au début une image de taille :\n",
    "\n",
    "- 32 * 32 * 3 -> max pooling\n",
    "- 16 * 16 * 3 -> max pooling\n",
    "- 8 * 8 * 3 -> max pooling\n",
    "- 4 * 4 * 3 -> max pooling\n",
    "- 2 * 2 * 3 -> max pooling\n",
    "- 1 * 1 * 3\n",
    "\n",
    "Notre réseau doit donc à partir d'une image 1 * 1 * 3(Rouge, vert, bleu) déterminer le type d'image selon 10 classes.\n",
    "Cela ne semble pas très efficace, je décide donc de supprimer les couches de 32 et 16 filtres afin de finir avec une image 4 * 4 * 3\n",
    "\n",
    "<img src=\"img/CNN_37.png\">\n",
    "\n",
    "<img src=\"img/CNN_38.png\">\n",
    "\n",
    "<img src=\"img/CNN_39.png\">\n",
    "\n",
    "<img src=\"img/CNN_40.png\">\n",
    "\n",
    "Le modèle permet d'atteindre 81 % sur le testing, cependant il overfit encore.\n",
    "Essayons une nouvelle approche :\n",
    "* On supprime le dropout et le max pooling après la dernière couche de convolution de 64 filtres\n",
    "* On augmente le dropout de 10 à 15 % pour combattre l'overfitting\n",
    "\n",
    "<img src=\"img/CNN_41.png\">\n",
    "\n",
    "<img src=\"img/CNN_42.png\">\n",
    "\n",
    "<img src=\"img/CNN_43.png\">\n",
    "\n",
    "<img src=\"img/CNN_44.png\">\n",
    "\n",
    "Comme on me voit, on commence à overfit dès le 5ème époch ce qui nous empèche d'obtenir des meilleurs performances.\n",
    "L'augmentation de l'overfitting est du au fait que le modèle est beaucoup trop complexe en parti avec maintenant une image de 8 x 8 x 3 en sortie.\n",
    "\n",
    "Il nous reste donc 2 solutions :\n",
    "* augmenter le nombre de données\n",
    "* utiliser des techniques de régularisation\n",
    "\n",
    "On va donc utiliser la data augmentation mais avant ça on va faire un dernier test avec un modèle de CNN moins complexe pour limiter l'overfitting et voir combien il est possible d'obtenir sans ajout de nouvelles images.\n",
    "\n",
    "Le test est effectué sans dropout pour voir à quel point l'augmentation du nombre de données va permettre de combattre l'overfitting.\n",
    "\n",
    "<img src=\"img/CNN_45.png\">\n",
    "\n",
    "<img src=\"img/CNN_46.png\">\n",
    "\n",
    "<img src=\"img/CNN_47.png\">\n",
    "\n",
    "<img src=\"img/CNN_48.png\">\n",
    "\n",
    "On a réussi à retarder l'overfitting mais celui-ci a bien eu lieu et on a eu seulement 76 % au testing.\n",
    "\n",
    "Essayons une nouvelle méthode de généralisation : batch normalization\n",
    "\n",
    "<img src=\"img/CNN_49.png\">\n",
    "\n",
    "<img src=\"img/CNN_50.png\">\n",
    "\n",
    "<img src=\"img/CNN_51.png\">\n",
    "\n",
    "<img src=\"img/CNN_52.png\">\n",
    "\n",
    "Comme on le voit la data normalization permet d'accélérer l'apprentissage et une meilleure généralisation(passage de 76 à 78 sur le testing)\n",
    "\n",
    "Maintenant on va essayer la data augmentation, cela consiste à prendre chaque image et à en créer des variantes, il y a plusieurs options :\n",
    "\n",
    "* Les images sont retournées sur les axes horizontaux et verticaux(images à l'envers)\n",
    "* l'image est zoomée par rapport à l'originale\n",
    "* les images subissent des rotations random\n",
    "* changement de contraste random dans les couleurs\n",
    "* ...\n",
    "\n",
    "On va prendre en parmaètre une donnée d'entrainement puis la modifier via data augmentation, cela va permettre au réseau de neurones de découvrir une nouvelle image à chaque itération.\n",
    "\n",
    "Premier Test avec data augmentation :\n",
    "\n",
    "<img src=\"img/CNN_53.png\">\n",
    "\n",
    "<img src=\"img/CNN_54.png\">\n",
    "\n",
    "<img src=\"img/CNN_55.png\">\n",
    "\n",
    "<img src=\"img/CNN_56.png\">\n",
    "\n",
    "Pic à 94,39 % sur le testing et 94, 41 % pour le training.\n",
    "La data augmentation permet une très bonne généralisation.\n",
    "\n",
    "Nouveau test :\n",
    "* En ajoutant plus de filtre pour les couches de convolution\n",
    "\n",
    "\n",
    "<img src=\"img/CNN_57.png\">\n",
    "\n",
    "<img src=\"img/CNN_58.png\">\n",
    "\n",
    "<img src=\"img/CNN_59.png\">\n",
    "\n",
    "<img src=\"img/CNN_60.png\">\n",
    "\n",
    "On obtient un pic de performance pour le training de 97,10 % et 96,89 % pour le testing.\n",
    "Bien évidemment les données de test ne sont pas modifiées, on applique unniquement la data augmentation aux données d'entrainement.\n",
    "\n",
    "Pour la santé mentale de mon PC, je ne pense pas lancer de nouveaux test(6 heures pour l'entrainement), le callback Early stopping avec une patiente de 30 ayant écourté celui-ci. En l'omettant on aurait peut-être pu obtenir une performance de 97 % sur le testing.\n",
    "Il aurait également intéréssant de voir s'il aurait été possible de complexifier le modèle encore plus et jusqu'à où on aurait pu aller grâce aux techniques de batch normalization et de Data augmentation. 99 % possible ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
